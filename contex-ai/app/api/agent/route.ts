import { NextRequest, NextResponse } from "next/server";
import OpenAI from "openai";
import { ElevenLabsService } from "@/lib/elevenLabsService";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

/**
 * A POST route that takes a user command, sends it to GPT,
 * and returns an object with:
 *  - assistantMessage: the text response,
 *  - actions: an array of AgentAction objects,
 *  - audio: a base64-encoded TTS audio generated by ElevenLabs.
 */
export async function POST(req: NextRequest) {
  try {
    // 1) Parse the JSON body from the request
    const body = await req.json(); // Expected shape: { userCommand: string, pageContext?: string }
    const userCommand: string = body?.userCommand ?? "";
    const pageContext: string = body?.pageContext ?? "";

    // 2) Detailed system prompt instructing the LLM to return valid JSON only.
    const baseSystemPrompt = `
      You are an AI agent that receives user instructions to navigate a web UI.
      You should output a single JSON object with two keys:
        - "assistantMessage": a short message responding to the user
        - "actions": an array of action objects

      Each action is an object with:
        - type: "click" | "hover" | "type" | "navigate"
        - targetId: string (the controlId of the element)
        - text?: string (only for "type" action)
        - navigateUrl?: string (only for "navigate")

      Example:
      [
        { "type": "click", "targetId": "home-button" },
        { "type": "type", "targetId": "my-input", "text": "Hello" }
      ]

      # Additional instruction:
      If the user is on the landing page and wants to go to the Library page, do it by clicking
      the button with controlId "library-button". 
      For example, return:
      [
        { "type": "click", "targetId": "library-button" }
      ]
      Similarly, if the user wants to go to the Generator page, do it by clicking
      the button with controlId "generator-button".

      If the user wants to go back to the home page, do so by clicking the button with controlId "home-button".

      Respond only with valid JSON, no extra text.
    `;

    // 3) Merge the page context with the base system prompt.
    const finalSystemPrompt = `
        ${baseSystemPrompt}

        # Current Page Context (if any):
        ${pageContext}
        `;

    const userPrompt = `User command: "${userCommand}"`;

    // 4) Call the OpenAI Chat Completion API.
    const response = await openai.chat.completions.create({
      model: "gpt-4o-mini",
      temperature: 0,
      messages: [
        { role: "system", content: finalSystemPrompt },
        { role: "user", content: userPrompt },
      ],
    });

    // 5) Extract GPT response text.
    let assistantContent =
      response.choices[0]?.message?.content?.trim() || "[]";

    // 6) Attempt to parse the response as JSON.
    let parsedResult;
    try {
      parsedResult = JSON.parse(assistantContent);
    } catch {
      parsedResult = { assistantMessage: "", actions: [] };
    }

    const assistantMessage = parsedResult.assistantMessage || "";
    const actions = parsedResult.actions || [];

    // 7) Generate TTS audio using ElevenLabsService if there's an assistant message.
    let audioBase64 = "";
    if (assistantMessage) {
      try {
        const elevenLabsService = new ElevenLabsService(
          process.env.ELEVENLABS_API_KEY as string
        );
        const audioBuffer = await elevenLabsService.generateSpeechFromText(assistantMessage);
        audioBase64 = audioBuffer.toString("base64");
      } catch (ttsError: any) {
        console.error("Error generating TTS audio:", ttsError);
      }
    }

    return NextResponse.json({ assistantMessage, actions, audio: audioBase64 });
  } catch (error: any) {
    console.error("Error in POST route:", error);
    return NextResponse.json({ error: error.message }, { status: 500 });
  }
}
